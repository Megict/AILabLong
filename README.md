# AILabLong

Я хочу решить задачу классификации частей речи в русском языке. Классические словари могут не содержать нужного слова, а также, поиск по таблицам словаря может занимать продолжительное время.  Если же обучить классификатор на заранее размеченных данных, можно обойти эти две проблемы. Так как в русском языке часть речи задается морфемами, т.е. разные части речи имеют различные характерные особенности в структуре слова, я считаю, что задачу определения части речи можно решить с достаточно высокой точностью, даже без учета контекста, в котором слово употреблено. Такой классификатор может найти применение в решении различных задач обработки естественного языка (NLP).  
Ввиду того, что различные характерные для частей речи морфемы не так многочисленны и разнообразны, в принципе, можно создать такой классификатор с помощью классического программирования. Так что, одна из основных моих задач – сравнить точность классификатора, основанного на методах машинного обучения и вручную запрограммированного классификатора. По возможности – добиться более высокой точности классификатора – нейросети, чем у классификатора – программы.  
Перед подачей на вход классификатора, слово представляется массивом символов. Размер массива равен размеру самого длинного слова обучающей выборки. Если слово короче массива, в пустые ячейки записываются символы, означающие отсутствие буквы. Какие – не принципиально. Каждая ячейка массива является индивидуальным признаком слова. Важно – удалить из обучающей выборки слова, сильно длиннее большего числа сильно длинных слов (чтобы не было такого, чтобы в последней ячейке массива буква была только у 1-2 слов). Массив подается на вход анализатору. Результатом работы анализатора будет являться число – номер класса. Классы: имя существительное [1], имя прилагательное [2], имя числительное [3], местоимение [4], глагол [5], наречие [6], причастие [7], предлог[8], союз [9], частица [10], междометие [11].  
Я планирую использовать либо датасет nerus(https://github.com/natasha/nerus), либо анализатор RuWord2Tags (https://github.com/Koziev/ruword2tags), который размечает слова на основе данных русского грамматического словаря. Правда, данная фраза из readme последнего проекта: «Кроме того, для многих новых (out-of-vocabulary) слов пакет обеспечивает распознавание, даже если точно такого слова нет в словаре.» – заставляет меня подозревать, что в нем уже имплементировано нечто, подобное тому, что я планирую разработать. Не знаю, насколько это плохо.  
##Вывод  
Пока не знаю, что тут написать, все-таки, пока что я лишь придумал идею и нашел датасеты, все самое интересно начнется, когда я попытаюсь что-то работающее создать. Пока что я засомневался, насколько хорошо подобная персептрону модель может работать с дискретными значениями признаков, ведь ситуация, когда значения признаков – буквы – не такая же, когда значения признаков – яркость пикселей. Возможно, это получится несколько обойти, сгруппировав буквы по типу «гласные» / «согласные», или, использовав иные предположения о структуре данных. В общем, вопрос применимости для решения данной задачи моделей машинного обучения также будет объектом исследования. Думаю, как-то, да получится тут их применить.  
